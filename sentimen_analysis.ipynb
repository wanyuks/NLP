{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据\n",
    "- TorchText中的一个重要概念是Field。Field决定了你的数据会被怎样处理。在我们的情感分类任务中，我们所需要接触到的数据有文本字符串和两种情感，\"pos\"和\"neg\"。\n",
    "- Field的参数指定了数据会被怎样处理。\n",
    "- 我们使用TEXT field来定义如何处理电影评论，使用LABEL field来处理两个情感类别。\n",
    "- LABEL是LabelField定义。这是一种特别用来处理label的Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import *\n",
    "from keras.datasets import imdb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 25000\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 64\n",
    "EMDEDDING_SIZE = 128\n",
    "HIDDEN_SIZE = 128\n",
    "DROP_OUT = 0.2\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad_sequences(sequences, maxlen=None, dtype=’int32’, padding=’pre’, truncating=’pre’, value=0.)\n",
    "\n",
    "sequences：浮点数或整数构成的两层嵌套列表\n",
    "\n",
    "maxlen：None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填0.\n",
    "\n",
    "dtype：返回的numpy array的数据类型\n",
    "\n",
    "padding：‘pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补\n",
    "\n",
    "truncating：‘pre’或‘post’，确定当需要截断序列时，从起始还是结尾截断\n",
    "\n",
    "value：浮点数，此值将在填充时代替默认的填充值0\n",
    "\n",
    "返回值\n",
    "返回形如(nb_samples,nb_timesteps)的2D张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 100) (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=VOCAB_SIZE)\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用DataLoader加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先将数据类型转化成torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.LongTensor(X_train), torch.LongTensor(y_train))\n",
    "test_data = TensorDataset(torch.LongTensor(X_test), torch.LongTensor(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后使用DataLoader加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[    1,    22,     2,  ...,    19,     4, 10591],\n",
       "         [    1,     4,   108,  ...,    14,  5351,   286],\n",
       "         [    1,    13,    86,  ...,  1420,  3935,   689],\n",
       "         ...,\n",
       "         [    1,     4,  3706,  ...,  1263,  6636,  2243],\n",
       "         [    1,   914,  1561,  ...,   848,  1532,  2402],\n",
       "         [    1,    14,    20,  ...,    10,    13,   124]]),\n",
       " tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "         0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "         1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, dropout, num_layers=5):\n",
    "        super(LstmModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dp = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embed(X)\n",
    "        X = self.dp(X)\n",
    "        X, _ = self.lstm(X)\n",
    "        X = self.dp(X)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.avg_pool2d(X, (X.shape[1], 1)).squeeze()\n",
    "        output = self.fc2(X)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义训练和测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    cri = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        y_ = model(x)\n",
    "        loss = cri(y_, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\"train loss: \",loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    cri = nn.CrossEntropyLoss(reduction='sum')\n",
    "    test_loss = 0.\n",
    "    acc = 0.\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.cuda(), y.cuda() \n",
    "        with torch.no_grad():\n",
    "            y_ = model(x)\n",
    "        test_loss += cri(y_, y)\n",
    "        pred = y_.max(-1, keepdim=True)[1]  # .max()分别输出最大值和最小值的index \n",
    "        acc += pred.eq(y.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(\"\\n Test Set: Average Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n",
    "    test_loss, acc, len(test_loader.dataset), 100. * acc / len(test_loader.dataset)))\n",
    "    model.train()\n",
    "    return acc / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  0.7021245360374451\n",
      "train loss:  0.6787962913513184\n",
      "train loss:  0.6805678606033325\n",
      "train loss:  0.5327714085578918\n",
      "\n",
      " Test Set: Average Loss: 0.4437, Accuracy: 19956.0/25000 (80%)\n",
      "acc is : 0.7982, beat acc is 0.7982\n",
      "\n",
      "train loss:  0.568015456199646\n",
      "train loss:  0.5468375086784363\n",
      "train loss:  0.4371986985206604\n",
      "train loss:  0.44487833976745605\n",
      "\n",
      " Test Set: Average Loss: 0.3666, Accuracy: 20863.0/25000 (83%)\n",
      "acc is : 0.8345, beat acc is 0.8345\n",
      "\n",
      "train loss:  0.3396390378475189\n",
      "train loss:  0.3420133888721466\n",
      "train loss:  0.35624340176582336\n",
      "train loss:  0.21501626074314117\n",
      "\n",
      " Test Set: Average Loss: 0.2425, Accuracy: 22849.0/25000 (91%)\n",
      "acc is : 0.9140, beat acc is 0.9140\n",
      "\n",
      "train loss:  0.18859118223190308\n",
      "train loss:  0.2604609429836273\n",
      "train loss:  0.3811567425727844\n",
      "train loss:  0.23622754216194153\n",
      "\n",
      " Test Set: Average Loss: 0.1704, Accuracy: 23386.0/25000 (94%)\n",
      "acc is : 0.9354, beat acc is 0.9354\n",
      "\n",
      "train loss:  0.13365298509597778\n",
      "train loss:  0.1913880556821823\n",
      "train loss:  0.19095708429813385\n",
      "train loss:  0.2328210175037384\n",
      "\n",
      " Test Set: Average Loss: 0.1115, Accuracy: 24052.0/25000 (96%)\n",
      "acc is : 0.9621, beat acc is 0.9621\n",
      "\n",
      "train loss:  0.09332738071680069\n",
      "train loss:  0.14385917782783508\n",
      "train loss:  0.13237786293029785\n",
      "train loss:  0.14394348859786987\n",
      "\n",
      " Test Set: Average Loss: 0.0770, Accuracy: 24424.0/25000 (98%)\n",
      "acc is : 0.9770, beat acc is 0.9770\n",
      "\n",
      "train loss:  0.11649660766124725\n",
      "train loss:  0.06053844466805458\n",
      "train loss:  0.18065381050109863\n",
      "train loss:  0.05488429218530655\n",
      "\n",
      " Test Set: Average Loss: 0.0523, Accuracy: 24638.0/25000 (99%)\n",
      "acc is : 0.9855, beat acc is 0.9855\n",
      "\n",
      "train loss:  0.08369921892881393\n",
      "train loss:  0.09398137032985687\n",
      "train loss:  0.06266777217388153\n",
      "train loss:  0.25926539301872253\n",
      "\n",
      " Test Set: Average Loss: 0.0273, Accuracy: 24836.0/25000 (99%)\n",
      "acc is : 0.9934, beat acc is 0.9934\n",
      "\n",
      "train loss:  0.08409838378429413\n",
      "train loss:  0.09939685463905334\n",
      "train loss:  0.041645411401987076\n",
      "train loss:  0.1419326514005661\n",
      "\n",
      " Test Set: Average Loss: 0.0173, Accuracy: 24920.0/25000 (100%)\n",
      "acc is : 0.9968, beat acc is 0.9968\n",
      "\n",
      "train loss:  0.05153487250208855\n",
      "train loss:  0.048302870243787766\n",
      "train loss:  0.0572320930659771\n",
      "train loss:  0.08529385179281235\n",
      "\n",
      " Test Set: Average Loss: 0.0095, Accuracy: 24958.0/25000 (100%)\n",
      "acc is : 0.9983, beat acc is 0.9983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LstmModel(vocab_size=VOCAB_SIZE, embed_size=EMDEDDING_SIZE, hidden_size=HIDDEN_SIZE, dropout=DROP_OUT)\n",
    "model= model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "best_acc = 0.\n",
    "for epoch in range(1, 11):\n",
    "    train(model=model, train_loader=train_loader, optimizer=optimizer, epoch=epoch)\n",
    "    acc = test(model=model, test_loader=test_loader)\n",
    "    if best_acc < acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), 'model/lstm_model.pth')\n",
    "    print(\"acc is : {:.4f}, beat acc is {:.4f}\\n\".format(acc, best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到最后的准确率达到了99.8%（PS：\"best\"写成了\"beat\"，请忽略）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
